## Attention in transformers
- The goal of the transformer model is to predict the next word in a sequence of words.
![Output examples:](../deepseek_assets/2.webp)

Tokens, which smallest units of text, are associated with high-dimensional vectors called embeddings.
![Output examples:](../deepseek_assets/3.webp)

These embeddings capture semantic meanig in a high-dimensional space.
![Output examples:](../deepseek_assets/4.1.webp)

![Output examples:](../deepseek_assets/5.webp)


