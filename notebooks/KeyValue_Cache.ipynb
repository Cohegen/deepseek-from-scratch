{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the Inference Bottleneck with Key-Value cache\n",
        ""
      ],
      "metadata": {
        "id": "8erO1qmxbCIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "* The inference bottleneck is one of the most critical challenges in deploying LLMs like deepseek and ChatGPT. This notebook is intended to explore how the key-value cache, a fundemental technique addresses this bottleneck and serves as the foundation for more advanced attention mechanisms.\n",
        "\n",
        "* In autoregressive generation, each token requires attention computations across previous tokens in the seqence. Without optimizations, this would lead to:\n",
        "  1. Quadratically increasing computation time as sequence lengths grows.\n",
        "  2. Redundant recomputation of key and value tensors for tokens that have already been processed.\n",
        "  3. Prohibitive memory and computational costs for practical applications.\n",
        "\n",
        "* The key-value cache solves these issues by storing previously computed key-value pairs dramatically reducing the computational burden during token generaton. This foundational technique enables Deepseek's impressive pefromance with long contexts of up to 128k tokens.\n",
        "\n",
        "\n",
        "## Multi-Head Attention\n",
        "* Deepsek uses multi-head attention,which allows the model attend to information from different representation subspaces simultaneously:\n",
        "\n",
        "`MultiHead(Q,K,V) = Concat(head_1....,head_h)W_O`\n",
        "* Where each head is computed as:\n",
        "`head_i = Attention(QWi_Q,KWi_K,VWi_V)`\n",
        "* This creates multiple \"attention heads\" that can focus on different aspects of the input sequence.\n",
        "\n",
        "## Autoregressive Generation: The Root of the Inference Bottleneck\n",
        "* Deepseek models, like other transformer-based LLMs, generate text autoregressively-one token at a time, where each new token depends on all previous tokens.\n",
        "* This creates a computational challenge during inference:\n",
        "  1. For the first token, we compute attention using just the prompt.\n",
        "  2. For the second token, we compute attention using the prompt plus the first generated token.\n",
        "  3. For the third token, we compute attention using all previous tokens.\n",
        "  4. And so on..\n",
        "\n",
        "* As the sequence grows, each new token requires more computation than the last. Without optimzation, this would create:\n",
        "   1. O(n^2) complexity in the sequence length for each new token.\n",
        "   2. Redundant calculations as the same keys and values are recomputed for existing tokens.\n",
        "   3. Slow inference speed for practical applications.\n",
        "\n",
        "* The following code demonstrates the autoregressive generation proess using a simple GPT-2."
      ],
      "metadata": {
        "id": "sntrPBUVbVjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "print(\"Models are being set up....\")\n",
        "#loading the pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "print(\"Setup completed successfully\")\n",
        "\n",
        "#visualizing autoregressive gpt-2 generation\n",
        "prompt = \"The Mona Lisa was painted\"\n",
        "inputs = tokenizer(prompt,return_tensors=\"pt\")\n",
        "input_ids = inputs.input_ids\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\",end=\"\")\n",
        "\n",
        "#generate 20 tokens\n",
        "for _ in range(20):\n",
        "  #pass the entire sequence to the model\n",
        "  outputs = model(input_ids)\n",
        "  logits = outputs.logits\n",
        "\n",
        "  #get the logits for the very last token\n",
        "  next_token_logits = logits[:,-1,:]\n",
        "\n",
        "  #get the ID of the most likely next token (greedy decoding)\n",
        "  next_token_id = torch.argmax(next_token_logits,dim=-1).unsqueeze(-1)\n",
        "\n",
        "  #append the new token ID to the input sequence\n",
        "  input_ids = torch.cat([input_ids,next_token_id],dim=-1)\n",
        "\n",
        "  #decode and print the new token\n",
        "  new_token = tokenizer.decode(next_token_id[0])\n",
        "  print(new_token,end=\"\",flush=True)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jZ8YCR8ddIr",
        "outputId": "66ebb0d6-eb51-4b6a-be40-3efbda55d17a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models are being set up....\n",
            "Setup completed successfully\n",
            "Prompt: 'The Mona Lisa was painted' in the same way as the original, but with a different color scheme.\n",
            "\n",
            "The Mona\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Mona Lisa was painted by\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "input_ids = inputs.input_ids\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# --- Timing without KV cache ---\n",
        "print(\"Generating without KV Cache...\")\n",
        "start_time_without_cache = time.time()\n",
        "output_without_cache = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=100,\n",
        "    use_cache=False, # Explicitly disable the cache\n",
        "    attention_mask=attention_mask\n",
        ")\n",
        "end_time_without_cache = time.time()\n",
        "duration_without_cache = end_time_without_cache - start_time_without_cache\n",
        "print(f\"Time without KV Cache: {duration_without_cache:.4f} seconds\\n\")\n",
        "\n",
        "\n",
        "# --- Timing with KV cache ---\n",
        "print(\"Generating with KV Cache...\")\n",
        "start_time_with_cache = time.time()\n",
        "output_with_cache = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=100,\n",
        "    use_cache=True, # Explicitly enable the cache\n",
        "    attention_mask=attention_mask\n",
        ")\n",
        "end_time_with_cache = time.time()\n",
        "duration_with_cache = end_time_with_cache - start_time_with_cache\n",
        "print(f\"Time with KV Cache: {duration_with_cache:.4f} seconds\\n\")\n",
        "\n",
        "\n",
        "# --- Calculate and print the speedup ---\n",
        "speedup = duration_without_cache / duration_with_cache\n",
        "print(f\"KV Cache Speedup: {speedup:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbg3Q-lZj0fH",
        "outputId": "6ac6aec4-37c9-4195-e1fc-27cbd5e2bc03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating without KV Cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time without KV Cache: 21.4571 seconds\n",
            "\n",
            "Generating with KV Cache...\n",
            "Time with KV Cache: 4.7570 seconds\n",
            "\n",
            "KV Cache Speedup: 4.51x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ffb6d7"
      },
      "source": [
        "This code cell demonstrates the autoregressive generation process. It initializes a GPT-2 model and tokenizer, then iteratively generates 20 tokens based on a given prompt. In each step, it feeds the entire sequence generated so far back into the model to predict the next token. This highlights the increasing computation required as the sequence grows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67e75f57"
      },
      "source": [
        "This code cell compares the inference time of generating text with and without the Key-Value (KV) cache. It uses the `model.generate` method with `use_cache=False` to disable the cache and `use_cache=True` to enable it. The execution time for each scenario is measured and the speedup achieved by using the KV cache is calculated and printed. This demonstrates the practical benefit of the KV cache in reducing inference time."
      ]
    }
  ]
}